# Unit galia.jl, part of groupLearning Package for julia
#
# MIT License 
# version: 10 Sept 2022
# Copyright (c) - 2023
# Fatih Altindis and Marco Congedo
# Abdullah Gul University, Kayseri
# GIPSA-lab, CNRS, University Grenoble Alpes

# This function implements a special joint blind source separation algorithm
# particularly suiting joint alignmment of multiple-dataset feature vectors.

# It takes as input a vector of M matrices, each one holding in columns the 
# feature vectors for each of M datasets (e.g., subjects).
# For the original data, the length of these vectors may be different for each dataset,
# however the number of features vectors must be the same for all datasets
# and should be at least equal to the highest feature vector length.
# If the length of these vectors is different, first one must whiten the M input matrices
# using M (whitening) matrices ğ–=[W[1], ..., W[m]] as it follows:
# T[m] = W[m]' * X[m], where W[m]' = w^-1 V', X[m] is the original data and X[m] = VwZ its SVD.
# Notice that the number of rows of matrices W[m]' is typically chosen so as to operate
# a dimensionality reduction (efficient Procrustes).  
# Finally, matrices ğ“=[T[1], ..., T[M]] are given as input of the function `joal`.
# If the length of these vectors is NOT different and no dimensionality reduction is sought,
# then the W[m] matrices are just the identity.

# `joal` outputs M matrices ğ”=[U[1],..., U[m]]. After running the algorithm all columns of all 
# matrices ğ” must be normalized to unit norm. 
# Then, the aligning matrices are obtained as B[m] = ğ”[m]' * ğ–[m]' for each m=1:M.

# These matrices are such that B[i] (T[i] T[j]') B[j]' are as diagonal
# as possible for all i â‰  j = 1:M 

# Given a full group learning model obtained on M subjects using the `joal` function,
# function `joal_newss` align a new subject to the M subjects of the full model.
# The function takes as input arguments (ğ“, ğ”, T), where
# ğ“ is the input of the `joal` function,
# ğ” is the output of the `joal` function and
# T is the whitened feature vectors matrix for the new subject.
# Let us denote the whitening matrix W.
# Note that the whitened T must have the same dimension as all matrices in ğ“.
# Function `joal_newss` outputs matrix U, which columns must be normalized to unit norm
# and from which then the alignment matrix for the new subject is computed as B = U' * W' 

# See : 
# Congedo M., BleuzÃ© A., Mattout J. (2022)
# Group Learning by Joint Alignment in the Riemannian Tangent Space
# GRETSI conference, 6-9 September 2022, Nancy, France.

# uses trifact! instead of julia cholesky and use BLAS functions
function jointAlignment(ğ“         ::AbstractArray;
                        init      :: Union{AbstractArray, Nothing} = nothing,
                        tol       :: Real = 0.,
                        maxiter   :: Int  = 1000,
                        verbose   :: Bool = false,
                        threaded  :: Bool = false)

    M, type, D = length(ğ“), eltype(ğ“[1]), size(ğ“[1], 1)
    iter, conv, oldconv, ğŸ˜‹, conv_ = 1, 0., 1.01, false, 0.
    diverging, previous_conv = false, 1.0
    tol==0. ? tolerance = âˆšeps(real(type)) : tolerance = tol            

    # thread operations only if n is at least Threads.nthreads() (no worth otherwise)
    threadedx = threaded && D â‰¥ Threads.nthreads() # !!!

    # Get input data C_ij,k (compute only lower triangular part of the matrix of matrices)
    ğ‚=[[Matrix{type}(undef, D, D) for i=1:j] for j=1:M]
    for j=1:M, i=j+1:M  
        ğ‚[i][j] = ğ“[i]* ğ“[j]' 
    end

    # use the initialization for matrices U if provided, otherwise initialize them to the identity 
    ğ” = init===nothing ? [Matrix{type}(I, D, D) for m=1:M] : init

    # pre-allocate memory
    V = Matrix{type}(undef, D, M - 1)
    ğ‘ = MatrixVector([Matrix{type}(undef, D, D) for d=1:D])

    verbose && @info("Iterating joal algorithm...")

    # This function is used only if threadedx is true
    function triS!(R, U, L, d)
        # y = L'\(L\BLAS.symv('L', R, U[:, d])) # BLAS call computes ğ‘[Î·]*ğ”[i][:, Î·]
        y = L'\(L\R*U[:, d])
        U[:, d] = y/âˆš(PosDefManifold.quadraticForm(y, R))
    end

    # Iterations
    while true
        conv_ =0.
        @inbounds for e2=1:2, i=1:M # m optimizations for updating ğ”[1]...ğ”[m]
                                    # double loop to avoid oscillating convergence
            for d=1:D
                x = 1   
                for j=1:i-1
                    V[:, x] = ğ‚[i][j] * ğ”[j][:, d] 
                    x += 1   
                end 
                for j=i+1:M  
                    V[:, x] = ğ”[j][:, d]' * ğ‚[j][i]
                    x += 1 
                end

                # write the lower triangle of V * V'
                ğ‘[d] = threadedx ?  LowerTriangular(BLAS.syrk('L', 'N', 1.0, V)) : (V * V')
            end

            # do 1 power iteration
            L=trifact!(threadedx ? PosDefManifold.fVec(sum, ğ‘) : sum(ğ‘)) # Cholesky LL'of ğ‘[1]+...+ğ‘[n]

            # Solve Lx=ğ‘[Î·]*ğ”[i][:, Î·] for x and L'*y=x for y,
            # then updates the Î·th column of ğ”[i] as ğ”[i][:, Î·] <- y/sqrt(y'*ğ‘[Î·]*y)
            if threadedx 
                @threads for d=1:D triS!(ğ‘[d], ğ”[i], L, d) end 
            else
                for d=1:D
                    # computee ğ‘[d]*ğ”[i][:, d]
                    y =  threadedx ?    L'\(L\BLAS.symv('L', ğ‘[d], ğ”[i][:, d])) :
                                        L'\(L\(ğ‘[d]*ğ”[i][:, d]))
                    ğ”[i][:, d] = y/âˆš(PosDefManifold.quadraticForm(y, ğ‘[d])) # qf accepts low. tri. matrices
                end    
            end       

            # update convergence in a quick way # improve this !!
            conv_+=PosDefManifold.ss(ğ”[i])   
        end

        conv_/=(2*D^2*M) # 2 because of the double loop
        iter==1 ? conv=1. : conv = abs((conv_-oldconv)/oldconv)  # relative change
        
        verbose && println("iteration: ", iter, "; convergence: ", conv)
        previous_conv < 1e-5 && !diverging ? (diverging = previous_conv < conv) && verbose && @warn("NoJoB diverged at:", iter) : nothing
        (overRun = iter == maxiter) && @warn("NoJoB: reached the max number of iterations before convergence:", iter)
        (ğŸ˜‹ = 0. <= conv <= tolerance) || overRun == true ? break : nothing
        previous_conv = deepcopy(conv)
        oldconv=conv_
        iter += 1
    end # while

    return ğ”, iter, conv, diverging # NB: columns have not unit norm
end
joal = jointAlignment; # alias

function fastAlignment(  ğ“::AbstractArray, ğ”::AbstractArray, T::Matrix;
                                threaded  :: Bool = false)

    M, type, D = length(ğ“), eltype(ğ“[1]), size(ğ“[1], 1)

    # thread operations only if n is at least Threads.nthreads() (no worth otherwise)
    threadedx = threaded && D â‰¥ Threads.nthreads() # !!!

    # Get input data C_ij
    ğ‚ = [T * ğ“[j]' for j=1:M]

     # pre-allocate memory
    V = Matrix{type}(undef, D, M)
    ğ‘ = MatrixVector([Matrix{type}(undef, D, D) for d=1:D])
    U = Matrix{type}(undef, D, D) 

    for d=1:D
        for j=1:M
            V[:, j] = ğ‚[j]*ğ”[j][:, d]
        end  
        ğ‘[d] = Hermitian(V * V')
    end

    H = invsqrt(Hermitian(sum(ğ‘)))

    for d=1:D    
        ev=eigvecs(Hermitian(H * ğ‘[d] * H'))
        U[:, d] = H * ev[:, end]
    end  # NB: the columns of U are not in unit norm   

    return U
end
faal = fastAlignment; # alias

function trifact!(	P::Matrix{T};
                    check::Bool = true,
                    tol::Real = âˆšeps(real(T))) where T<:Union{Real}

    LinearAlgebra.require_one_based_indexing(P)
    n = LinearAlgebra.checksquare(P)

    @inbounds for j=1:n-1
        check && abs2(P[j, j])<tol && throw(LinearAlgebra.PosDefException(1))
        f = P[j, j]
        P[j, j] = âˆš(f)
        g = P[j, j]
        for i=j+1:n
            Î¸ = P[i, j] / f
            cÎ¸ = conj(Î¸)
            for k=i:n P[k, i] -= cÎ¸ * P[k, j] end # update P and write D
            P[i, j] = Î¸ * g # write L factor
        end
    end
    P[n, n] = âˆš(P[n, n]) # write last diagonal element of L factor

    return LowerTriangular(P)
end

function joalInitializer(ğ“     :: Vector{Matrix{Float64}};
                         type  :: Symbol = :smart)

    M = length(ğ“)
    U_init = [Matrix{Float64}(I, size(ğ“[1], 1), size(ğ“[1], 1)) for m=1:M]
    if type == :smart
        ğ‚ = [[Matrix{}(undef, size(ğ“[1], 1), size(ğ“[1], 1)) for i=1:j] for j=1:M]
        for j=1:M, i=j+1:M  
            ğ‚[i][j] = ğ“[i] * ğ“[j]' 
        end

        H = Matrix{}(undef, size(ğ“[1], 1), size(ğ“[1], 1))
        for m=1:M
            fill!(H,0)
            for j=1:M
                if m>j 
                    H += ğ‚[m][j]
                end 
                if m<j 
                    H += ğ‚[j][m]'
                end 
            end 
            F = svd(H, alg=LinearAlgebra.QRIteration())
            U_init[m] = F.U
        end
        return U_init
    else
        return nothing
    end
end
